<!DOCTYPE html>
<html lang="en"><head>
<script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/clipboard/clipboard.min.js"></script>
<script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/quarto-html/tabby.min.js"></script>
<script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/quarto-html/popper.min.js"></script>
<script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.333">

  <meta name="author" content="Peter Gilles">
  <meta name="dcterms.date" content="2023-05-10">
  <title>Automatic Speech recognition of Luxembourgish</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Automatic Speech recognition of Luxembourgish</h1>
  <p class="subtitle">Lecture ‘Data Science in the Humanities’</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Peter Gilles 
</div>
</div>
</div>

  <p class="date">2023-05-10</p>
</section>
<section id="structure-of-the-lecture" class="slide level2">
<h2>Structure of the lecture</h2>
<ol type="1">
<li>Introduction</li>
<li>Dealing with audio data</li>
<li>Basics of ASR</li>
<li>ASR for a small language like Luxembourgish</li>
<li>Meta’s wav2vec 2.0</li>
<li>OpenAi’s Whisper</li>
<li>Conclusion</li>
</ol>
</section>
<section id="introdcution-history-of-automatic-speech-recognition" class="slide level2">
<h2>Introdcution: History of Automatic Speech Recognition</h2>
<ul>
<li>Euphoric phase in the 1950s at Bell Labs, IBM</li>
<li>Hidden Markov Models (HMM)
<ul>
<li>predict the probability of a sound or a word</li>
<li>not based on acoustic features alone</li>
</ul></li>
<li>1980s: Neural Networks
<ul>
<li>Loads of annotated training material needed (based on word or even phonetic segment)</li>
<li>Only for one specific language</li>
<li>Highly task and speaker dependent</li>
</ul></li>
<li>2010s: Deep Neural Networks / Transformer Models</li>
</ul>
</section>
<section id="intro-applications-of-asr" class="slide level2">
<h2>Intro: Applications of ASR</h2>
<ul>
<li>Real-time transcription
<ul>
<li>dictation</li>
<li>subtitling of videos</li>
</ul></li>
<li>Transcription of audio recordings
<ul>
<li>Speeches, debates at the parliament, podcasts, videos etc.</li>
</ul></li>
<li>Audio commands
<ul>
<li>Siri, Alexa</li>
</ul></li>
<li>Keyword detection (i.e.&nbsp;for call centers)</li>
<li>Speaker verification / identification</li>
<li>Phonetic segmentation of audio recordings (‘Alignment’) for linguistic studies</li>
<li>Emotion detection</li>
<li>AI interactions
<ul>
<li>chatbots</li>
<li>Q&amp;A</li>
</ul></li>
</ul>
</section>
<section id="dealing-with-audio-data" class="slide level2">
<h2>Dealing with audio data</h2>
<ul>
<li><p>Recommended overview: <span class="citation" data-cites="huiSpeechRecognitionPhonetics2022">Hui (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span></p></li>
<li><p>Acoustic representation of speech: continuous signal with amplitude modulation in time <img data-src="Speech%20Recognition%20-%20Data%20Science%20and%20the%20Humanities%20-%202023_files/figure-revealjs/180f0b35-9386-49bf-8e4b-5459668ce62c-1-1628698b-05e0-433b-93c3-b80c07282edb.png" alt="waveform"></p></li>
<li><p>Incredibly variable, due to …</p>
<ul>
<li>accent</li>
<li>speaking style</li>
<li>speaker-dependent characteristics</li>
<li>recording conditions</li>
</ul></li>
<li><p>Basic acoustic representation for speech: Spectrogram</p>
<ul>
<li>x: time</li>
<li>y: frequency</li>
<li>color: intensity</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="Speech%20Recognition%20-%20Data%20Science%20and%20the%20Humanities%20-%202023_files/figure-revealjs/180f0b35-9386-49bf-8e4b-5459668ce62c-2-9634cd64-767d-4da9-a152-75055119c352.png"></p>
<figcaption>spectrogram</figcaption>
</figure>
</div>
</section>
<section id="dealing-with-audio-data-1" class="slide level2">
<h2>Dealing with audio data</h2>
<ul>
<li>Processing unit for ASR:
<ul>
<li>frames with 25ms width</li>
<li>10ms overlap</li>
<li>capture all relevant linguistic information</li>
<li>feature extraction: Mel-frequency cepstral coefficients (MFCC) or Mel spectrogram</li>
<li>39 features per frame, stored as vector <img data-src="Speech%20Recognition%20-%20Data%20Science%20and%20the%20Humanities%20-%202023_files/figure-revealjs/a382c2c7-ec76-4d96-b016-1fb2b4d507cb-1-38aba670-424c-4143-843e-97113c382d4b.png" alt="from Hui (2019)"></li>
</ul></li>
</ul>
</section>
<section id="basics-of-automatic-speech-recognition" class="slide level2">
<h2>Basics of Automatic Speech Recognition</h2>
<ul>
<li>Main task of ASR
<ul>
<li>Use a statistical model to recognise the correct text from a sequence of feature vectors</li>
</ul></li>
<li>Design of a traditional ASR system <img data-src="Speech%20Recognition%20-%20Data%20Science%20and%20the%20Humanities%20-%202023_files/figure-revealjs/e8ddec01-3648-4295-84ea-60c944da1611-1-72475551-1146-48be-9f0b-d0a17d290b2e.png" style="width:80.0%"></li>
<li>Necessary ingredients for the model
<ul>
<li>acoustic model based on fine-grained annotated speech</li>
<li>pronunciation lexicon</li>
<li>language model</li>
</ul></li>
</ul>
</section>
<section id="basics-of-automatic-speech-recognition-1" class="slide level2">
<h2>Basics of Automatic Speech Recognition</h2>
<ul>
<li><p>The rise of Deep Learning (DL) <img data-src="Speech%20Recognition%20-%20Data%20Science%20and%20the%20Humanities%20-%202023_files/figure-revealjs/643d0c9c-0319-44a4-88d5-2277d24e6db1-1-ae1b1b1c-a4f3-4cb2-b5fa-1cd2da84e6d0.png" alt="Boigne (2021)"></p></li>
<li><p>Transformer models</p>
<ul>
<li>accounting for context in a broad and flexible way</li>
<li>abstract and generalized representations of the speech signal that capture its relevant features, such as phonetic, spectral, or contextual information</li>
<li>generated in a Neural Network</li>
</ul></li>
<li><p>End-to-end models</p>
<ul>
<li>no intermediate steps necessary, e.g.&nbsp;phoneme detection, phoneme2grapheme conversion, pronunciation dictionary etc.</li>
</ul></li>
</ul>
</section>
<section id="asr-for-low-resource-languages" class="slide level2">
<h2>ASR for low-resource languages</h2>
<ul>
<li>Challenges of Luxembourgish
<ul>
<li>paucity of training data</li>
<li>lots of spelling variation</li>
<li>costly to compile training data</li>
</ul></li>
</ul>
</section>
<section id="compilation-of-training-data" class="slide level2">
<h2>Compilation of training data</h2>
<ul>
<li>Algorithms will ‘learn’ from example
<ul>
<li>pairs of text transcription and matching audio</li>
<li>compilation of audio samples of 2 to 20 sec length</li>
<li>multiple speakers</li>
<li>various speaking genres (debates in the parliament, media recordings, podcasts, interviews, elicited sentences etc.)</li>
<li>meticulous checks of the data</li>
<li>atm 40,000 samples / 70 hours of training data</li>
</ul></li>
<li>Training data shall contain as many sound and word combinations as possible</li>
</ul>
<table>
<colgroup>
<col style="width: 54%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Transcription</th>
<th>Audio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Luc hat eng Frëndin, déi lieft a Corse.</td>
<td><audio src="https://luxappsdata.uni.lu/schnessen/media/recording_recordings/2018-08-06/pg_191_3314_p1rzb.wav" controls=""> </audio></td>
</tr>
<tr class="even">
<td>hei iwwer déi Datumen ze schwätzen. Ech wëll just soen, datt ech e Rapport vum 16.</td>
<td><audio src="Chamber_2020_10_29_1917.wav" controls=""> </audio></td>
</tr>
<tr class="odd">
<td>Entschëllegt, et ass déi, vun där ech och schwätzen. A mir kënnen net äh am Kaffismarc liesen äh</td>
<td><audio src="Chamber_2020_10_29_1913.wav" controls=""> </audio></td>
</tr>
<tr class="even">
<td>Wat mer matgedeelt kréien. Mir hunn dat Resultat eréischt matgedeelt kritt, nodeem et</td>
<td><audio src="Chamber_2020_10_29_1914.wav" controls=""> </audio></td>
</tr>
</tbody>
</table>
</section>
<section id="metas-wav2vec-2.0-baevskiwav2vecframeworkselfsupervised2020" class="slide level2">
<h2>Meta’s Wav2vec 2.0 (<span class="citation" data-cites="baevskiWav2vecFrameworkSelfSupervised2020">Baevski et al. (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span>)</h2>
<ul>
<li>Self-supervised learning of unlabeled speech</li>
<li>Trained on 0.5 million hours of speech from 128 languages</li>
<li>Pre-training with up to 1 billion of parameters</li>
<li>Highly useful for low-resource languages by building upon similar structures from related languages, i.e.&nbsp;German, Dutch, French</li>
<li>Components of Wav2vec 2.0 <img data-src="Speech%20Recognition%20-%20Data%20Science%20and%20the%20Humanities%20-%202023_files/figure-revealjs/a1bc37ab-9c55-4ccc-86c0-c28515663f7a-1-6c0dd04c-d39e-4538-a764-6426877a4064.png" alt="“Fine-Tune Wav2Vec2 for English ASR in Hugging Face with 🤗 Transformers” (n.d.)"></li>
</ul>
</section>
<section id="preprocessing-for-fine-tuning" class="slide level2">
<h2>Preprocessing for fine-tuning</h2>
<ul>
<li>Fine-tuning
<ul>
<li>adding a specific layer to the pretrained self-supervised model</li>
<li>new training data will enable the model to ‘learn’ Luxembourgish</li>
</ul></li>
<li>preparation of the training data for training
<ul>
<li>several cleaning steps of the text data</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Load raw data for training</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>df <span class="op">=</span>pd.read_csv(<span class="st">"./lux_train.csv"</span>)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co">#df = pd.read_csv("./lux_train.tsv", sep='\t')</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>df[<span class="st">'sentence'</span>]<span class="op">=</span>df[<span class="st">'sentence'</span>].<span class="bu">apply</span>(<span class="bu">str</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># replace numbers to text</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>df[<span class="st">'sentence'</span>]<span class="op">=</span>df[<span class="st">'sentence'</span>].<span class="bu">apply</span>(replaceInt)</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co"># define train and test split</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>train, test <span class="op">=</span> train_test_split(df, test_size<span class="op">=</span><span class="fl">0.15</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a>lux_voice_train <span class="op">=</span> Dataset.from_pandas(train)</span>
<span id="cb1-12"><a href="#cb1-12"></a>lux_voice_test <span class="op">=</span> Dataset.from_pandas(test)</span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="bu">print</span>(<span class="st">"Data loaded"</span>)</span>
<span id="cb1-15"><a href="#cb1-15"></a></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="bu">print</span>(<span class="st">"Starting cleaning"</span>)</span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="co"># Cleaning of text data</span></span>
<span id="cb1-18"><a href="#cb1-18"></a>chars_to_ignore_regex <span class="op">=</span> <span class="st">'[\,\?\.\!\+\;\:</span><span class="ch">\"</span><span class="st">\“\&amp;\‘\”\�\(\)\&lt;\&gt;\«\»\„]'</span></span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="co"># convert, also to lower case!</span></span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="kw">def</span> remove_special_characters(batch):</span>
<span id="cb1-22"><a href="#cb1-22"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(chars_to_ignore_regex, <span class="st">''</span>, batch[<span class="st">"sentence"</span>]).lower() <span class="op">+</span> <span class="st">" "</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>    <span class="cf">return</span> batch</span>
<span id="cb1-24"><a href="#cb1-24"></a></span>
<span id="cb1-25"><a href="#cb1-25"></a>lux_voice_train <span class="op">=</span> lux_voice_train.<span class="bu">map</span>(remove_special_characters)</span>
<span id="cb1-26"><a href="#cb1-26"></a>lux_voice_test <span class="op">=</span> lux_voice_test.<span class="bu">map</span>(remove_special_characters)</span>
<span id="cb1-27"><a href="#cb1-27"></a></span>
<span id="cb1-28"><a href="#cb1-28"></a><span class="co"># replacing special characters</span></span>
<span id="cb1-29"><a href="#cb1-29"></a><span class="kw">def</span> replace_hatted_characters(batch):</span>
<span id="cb1-30"><a href="#cb1-30"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[â]'</span>, <span class="st">'a'</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-31"><a href="#cb1-31"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[î]'</span>, <span class="st">'i'</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-32"><a href="#cb1-32"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[ô]'</span>, <span class="st">'o'</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-33"><a href="#cb1-33"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[û]'</span>, <span class="st">'u'</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-34"><a href="#cb1-34"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[ê]'</span>, <span class="st">'e'</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-35"><a href="#cb1-35"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[ï]'</span>, <span class="st">'i'</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-36"><a href="#cb1-36"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[ή]'</span>, <span class="st">'n'</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-37"><a href="#cb1-37"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[ß]'</span>, <span class="st">'ss'</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-38"><a href="#cb1-38"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[ñ]'</span>, <span class="st">'n'</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-39"><a href="#cb1-39"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[/]'</span>, <span class="st">' '</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-40"><a href="#cb1-40"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[’]'</span>, <span class="st">'</span><span class="ch">\'</span><span class="st">'</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-41"><a href="#cb1-41"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[°]'</span>, <span class="st">' grad '</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-42"><a href="#cb1-42"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[%]'</span>, <span class="st">' prozent '</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-43"><a href="#cb1-43"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[@]'</span>, <span class="st">' '</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-44"><a href="#cb1-44"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[#]'</span>, <span class="st">' '</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-45"><a href="#cb1-45"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[_]'</span>, <span class="st">' '</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-46"><a href="#cb1-46"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[–]'</span>, <span class="st">' '</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-47"><a href="#cb1-47"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[-]'</span>, <span class="st">' '</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-48"><a href="#cb1-48"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[</span><span class="ch">\\</span><span class="st">xad]'</span>, <span class="st">' '</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-49"><a href="#cb1-49"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'[</span><span class="ch">\\</span><span class="st">x01]'</span>, <span class="st">' '</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-50"><a href="#cb1-50"></a>    batch[<span class="st">"sentence"</span>] <span class="op">=</span> re.sub(<span class="st">'  '</span>, <span class="st">' '</span>, batch[<span class="st">"sentence"</span>])</span>
<span id="cb1-51"><a href="#cb1-51"></a>    <span class="cf">return</span> batch</span>
<span id="cb1-52"><a href="#cb1-52"></a></span>
<span id="cb1-53"><a href="#cb1-53"></a>lux_voice_train <span class="op">=</span> lux_voice_train.<span class="bu">map</span>(replace_hatted_characters)</span>
<span id="cb1-54"><a href="#cb1-54"></a>lux_voice_test <span class="op">=</span> lux_voice_test.<span class="bu">map</span>(replace_hatted_characters)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="preprocessing-for-fine-tuning-1" class="slide level2">
<h2>Preprocessing for fine-tuning</h2>
<ul>
<li>Extract all distinct letters</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> extract_all_chars(batch):</span>
<span id="cb2-2"><a href="#cb2-2"></a>  all_text <span class="op">=</span> <span class="st">" "</span>.join(batch[<span class="st">"sentence"</span>])</span>
<span id="cb2-3"><a href="#cb2-3"></a>  vocab <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(all_text))</span>
<span id="cb2-4"><a href="#cb2-4"></a>  <span class="cf">return</span> {<span class="st">"vocab"</span>: [vocab], <span class="st">"all_text"</span>: [all_text]}</span>
<span id="cb2-5"><a href="#cb2-5"></a></span>
<span id="cb2-6"><a href="#cb2-6"></a>vocab_train <span class="op">=</span> lux_voice_train.<span class="bu">map</span>(extract_all_chars, batched<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=-</span><span class="dv">1</span>, keep_in_memory<span class="op">=</span><span class="va">True</span>, remove_columns<span class="op">=</span>lux_voice_train.column_names)</span>
<span id="cb2-7"><a href="#cb2-7"></a>vocab_test <span class="op">=</span> lux_voice_test.<span class="bu">map</span>(extract_all_chars, batched<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=-</span><span class="dv">1</span>, keep_in_memory<span class="op">=</span><span class="va">True</span>, remove_columns<span class="op">=</span>lux_voice_test.column_names)</span>
<span id="cb2-8"><a href="#cb2-8"></a></span>
<span id="cb2-9"><a href="#cb2-9"></a>vocab_list <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(vocab_train[<span class="st">"vocab"</span>][<span class="dv">0</span>]) <span class="op">|</span> <span class="bu">set</span>(vocab_test[<span class="st">"vocab"</span>][<span class="dv">0</span>]))</span>
<span id="cb2-10"><a href="#cb2-10"></a></span>
<span id="cb2-11"><a href="#cb2-11"></a>vocab_dict <span class="op">=</span> {v: k <span class="cf">for</span> k, v <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">sorted</span>(vocab_list))}</span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="bu">print</span>(vocab_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Vocab dict</li>
</ul>
<pre><code>{' ': 0, '$': 1, "'": 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, '\\': 13, '^': 14, 'a': 15, 'b': 16, 'c': 17, 'd': 18, 'e': 19, 'f': 20, 'g': 21, 'h': 22, 'i': 23, 'j': 24, 'k': 25, 'l': 26, 'm': 27, 'n': 28, 'o': 29, 'p': 30, 'q': 31, 'r': 32, 's': 33, 't': 34, 'u': 35, 'v': 36, 'w': 37, 'x': 38, 'y': 39, 'z': 40, 'à': 41, 'á': 42, 'ä': 43, 'ç': 44, 'è': 45, 'é': 46, 'ë': 47, 'ö': 48, 'ú': 49, 'ü': 50, 'œ': 51, '‚': 52, '…': 53}</code></pre>
</section>
<section id="preprocessing-for-fine-tuning-2" class="slide level2">
<h2>Preprocessing for fine-tuning</h2>
<ul>
<li>Convert the audio data to vectors</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">def</span> speech_file_to_array_fn(batch):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    speech_array, sampling_rate <span class="op">=</span> sf.read(batch[<span class="st">"path"</span>])</span>
<span id="cb4-3"><a href="#cb4-3"></a>    batch[<span class="st">"speech"</span>] <span class="op">=</span> speech_array</span>
<span id="cb4-4"><a href="#cb4-4"></a>    batch[<span class="st">"sampling_rate"</span>] <span class="op">=</span> sampling_rate</span>
<span id="cb4-5"><a href="#cb4-5"></a>    batch[<span class="st">"target_text"</span>] <span class="op">=</span> batch[<span class="st">"sentence"</span>]</span>
<span id="cb4-6"><a href="#cb4-6"></a>    <span class="cf">return</span> batch</span>
<span id="cb4-7"><a href="#cb4-7"></a></span>
<span id="cb4-8"><a href="#cb4-8"></a>lux_voice_train <span class="op">=</span> lux_voice_train.<span class="bu">map</span>(speech_file_to_array_fn, remove_columns<span class="op">=</span>lux_voice_train.column_names,num_proc<span class="op">=</span><span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Prepare the model</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">from</span> transformers <span class="im">import</span> Wav2Vec2CTCTokenizer</span>
<span id="cb5-2"><a href="#cb5-2"></a>tokenizer <span class="op">=</span> Wav2Vec2CTCTokenizer(<span class="st">"./vocab.json"</span>, unk_token<span class="op">=</span><span class="st">"[UNK]"</span>, pad_token<span class="op">=</span><span class="st">"[PAD]"</span>, word_delimiter_token<span class="op">=</span><span class="st">"|"</span>)</span>
<span id="cb5-3"><a href="#cb5-3"></a></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="im">from</span> transformers <span class="im">import</span> Wav2Vec2FeatureExtractor</span>
<span id="cb5-5"><a href="#cb5-5"></a>feature_extractor <span class="op">=</span> Wav2Vec2FeatureExtractor(feature_size<span class="op">=</span><span class="dv">1</span>, sampling_rate<span class="op">=</span><span class="dv">16000</span>, padding_value<span class="op">=</span><span class="fl">0.0</span>, do_normalize<span class="op">=</span><span class="va">True</span>, return_attention_mask<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-6"><a href="#cb5-6"></a></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="im">from</span> transformers <span class="im">import</span> Wav2Vec2Processor</span>
<span id="cb5-8"><a href="#cb5-8"></a>processor <span class="op">=</span> Wav2Vec2Processor(feature_extractor<span class="op">=</span>feature_extractor, tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb5-9"><a href="#cb5-9"></a></span>
<span id="cb5-10"><a href="#cb5-10"></a>processor.save_pretrained(<span class="st">"./wav2vec2-large-xls-r-LUXEMBOURGISH_NEW_DATASET"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Preprocess the audio vectors and labels with the Wav2Vec2ForCTC processor</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">def</span> prepare_dataset(batch):</span>
<span id="cb6-2"><a href="#cb6-2"></a>    <span class="co"># check that all files have the correct sampling rate</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>    <span class="cf">assert</span> (</span>
<span id="cb6-4"><a href="#cb6-4"></a>        <span class="bu">len</span>(<span class="bu">set</span>(batch[<span class="st">"sampling_rate"</span>])) <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>    ), <span class="ss">f"Make sure all inputs have the same sampling rate of </span><span class="sc">{</span>processor<span class="sc">.</span>feature_extractor<span class="sc">.</span>sampling_rate<span class="sc">}</span><span class="ss">."</span></span>
<span id="cb6-6"><a href="#cb6-6"></a></span>
<span id="cb6-7"><a href="#cb6-7"></a>    batch[<span class="st">"input_values"</span>] <span class="op">=</span> processor(batch[<span class="st">"speech"</span>], sampling_rate<span class="op">=</span>batch[<span class="st">"sampling_rate"</span>][<span class="dv">0</span>]).input_values</span>
<span id="cb6-8"><a href="#cb6-8"></a>    </span>
<span id="cb6-9"><a href="#cb6-9"></a>    <span class="cf">with</span> processor.as_target_processor():</span>
<span id="cb6-10"><a href="#cb6-10"></a>        batch[<span class="st">"labels"</span>] <span class="op">=</span> processor(batch[<span class="st">"target_text"</span>]).input_ids</span>
<span id="cb6-11"><a href="#cb6-11"></a>    <span class="cf">return</span> batch</span>
<span id="cb6-12"><a href="#cb6-12"></a>    </span>
<span id="cb6-13"><a href="#cb6-13"></a>lux_voice_test <span class="op">=</span> lux_voice_test.<span class="bu">map</span>(prepare_dataset, remove_columns<span class="op">=</span>lux_voice_test.column_names, batch_size<span class="op">=</span><span class="dv">8</span>, num_proc<span class="op">=</span><span class="dv">8</span>, batched<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="training-i.e.-fine-tuning" class="slide level2">
<h2>Training, i.e.&nbsp;fine-tuning</h2>
<ul>
<li>Load the pre-trained model from Hugging Face</li>
<li>Load the preprocessed dataset</li>
<li>Define training parameters</li>
<li>Wait for 30 hours</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>model <span class="op">=</span> Wav2Vec2ForCTC.from_pretrained(</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="st">"facebook/wav2vec2-xls-r-300m"</span>, </span>
<span id="cb7-3"><a href="#cb7-3"></a>    <span class="co">#"facebook/wav2vec2-xls-r-2b", </span></span>
<span id="cb7-4"><a href="#cb7-4"></a>    attention_dropout<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb7-5"><a href="#cb7-5"></a>    hidden_dropout<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb7-6"><a href="#cb7-6"></a>    feat_proj_dropout<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb7-7"><a href="#cb7-7"></a>    mask_time_prob<span class="op">=</span><span class="fl">0.05</span>, <span class="co">#0.05</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>    layerdrop<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb7-9"><a href="#cb7-9"></a>    ctc_loss_reduction<span class="op">=</span><span class="st">"mean"</span>,</span>
<span id="cb7-10"><a href="#cb7-10"></a>    ctc_zero_infinity<span class="op">=</span><span class="va">True</span>, <span class="co">#Missing</span></span>
<span id="cb7-11"><a href="#cb7-11"></a>    pad_token_id<span class="op">=</span>processor.tokenizer.pad_token_id,</span>
<span id="cb7-12"><a href="#cb7-12"></a>    vocab_size<span class="op">=</span><span class="bu">len</span>(processor.tokenizer),</span>
<span id="cb7-13"><a href="#cb7-13"></a>)</span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="bu">print</span>(<span class="st">"Pretrained model loaded"</span>)</span>
<span id="cb7-15"><a href="#cb7-15"></a></span>
<span id="cb7-16"><a href="#cb7-16"></a>model.freeze_feature_encoder()</span>
<span id="cb7-17"><a href="#cb7-17"></a></span>
<span id="cb7-18"><a href="#cb7-18"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb7-19"><a href="#cb7-19"></a>  output_dir<span class="op">=</span><span class="st">"./wav2vec2-large-xls-r-LUXEMBOURGISH"</span>,</span>
<span id="cb7-20"><a href="#cb7-20"></a>  group_by_length<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-21"><a href="#cb7-21"></a>    adam_beta1<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb7-22"><a href="#cb7-22"></a>    adam_beta2<span class="op">=</span><span class="fl">0.998</span>,</span>
<span id="cb7-23"><a href="#cb7-23"></a>    adam_epsilon<span class="op">=</span><span class="fl">1e-6</span>,</span>
<span id="cb7-24"><a href="#cb7-24"></a>    weight_decay<span class="op">=</span><span class="fl">0.005</span>, <span class="co"># Leo: to try 0.01</span></span>
<span id="cb7-25"><a href="#cb7-25"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">18</span>, <span class="co">#16, 24, 32(TPU?)</span></span>
<span id="cb7-26"><a href="#cb7-26"></a>    gradient_accumulation_steps<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb7-27"><a href="#cb7-27"></a>    gradient_checkpointing<span class="op">=</span><span class="va">True</span>, <span class="co">#Missing</span></span>
<span id="cb7-28"><a href="#cb7-28"></a>    evaluation_strategy<span class="op">=</span><span class="st">"steps"</span>,</span>
<span id="cb7-29"><a href="#cb7-29"></a>    num_train_epochs<span class="op">=</span><span class="dv">30</span>, <span class="co"># Leo: 20, 30 epochs, 40, yqs  0.5 - Just for demo, change it</span></span>
<span id="cb7-30"><a href="#cb7-30"></a>    fp16<span class="op">=</span><span class="va">True</span>, <span class="co"># Leo: Just on CUDA devices</span></span>
<span id="cb7-31"><a href="#cb7-31"></a>    save_steps<span class="op">=</span><span class="dv">400</span>, <span class="co">#20  Just for demo, change it</span></span>
<span id="cb7-32"><a href="#cb7-32"></a>    eval_steps<span class="op">=</span><span class="dv">400</span>, <span class="co">#20  Just for demo, change it</span></span>
<span id="cb7-33"><a href="#cb7-33"></a>    logging_steps<span class="op">=</span><span class="dv">400</span>, <span class="co">#20  Just for demo, change it</span></span>
<span id="cb7-34"><a href="#cb7-34"></a>    learning_rate<span class="op">=</span><span class="fl">3e-4</span>, </span>
<span id="cb7-35"><a href="#cb7-35"></a>    warmup_steps<span class="op">=</span><span class="dv">800</span>, <span class="co">#400, 20 Just for demo, change it</span></span>
<span id="cb7-36"><a href="#cb7-36"></a>    save_total_limit<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb7-37"><a href="#cb7-37"></a>    metric_for_best_model<span class="op">=</span><span class="st">"eval_loss"</span>, <span class="co"># Leo for early stopping: eval_wer</span></span>
<span id="cb7-38"><a href="#cb7-38"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,  <span class="co"># Leo for early stopping</span></span>
<span id="cb7-39"><a href="#cb7-39"></a>    greater_is_better<span class="op">=</span><span class="va">False</span>, <span class="co"># Leo for early stopping</span></span>
<span id="cb7-40"><a href="#cb7-40"></a>  push_to_hub<span class="op">=</span><span class="va">False</span></span>
<span id="cb7-41"><a href="#cb7-41"></a>)</span>
<span id="cb7-42"><a href="#cb7-42"></a></span>
<span id="cb7-43"><a href="#cb7-43"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb7-44"><a href="#cb7-44"></a>    model<span class="op">=</span>model,</span>
<span id="cb7-45"><a href="#cb7-45"></a>    data_collator<span class="op">=</span>data_collator,</span>
<span id="cb7-46"><a href="#cb7-46"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb7-47"><a href="#cb7-47"></a>    compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb7-48"><a href="#cb7-48"></a>    train_dataset<span class="op">=</span>common_voice_train,</span>
<span id="cb7-49"><a href="#cb7-49"></a>    eval_dataset<span class="op">=</span>common_voice_test,</span>
<span id="cb7-50"><a href="#cb7-50"></a>    tokenizer<span class="op">=</span>processor.feature_extractor</span>
<span id="cb7-51"><a href="#cb7-51"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="attaching-a-language-model" class="slide level2">
<h2>Attaching a language model</h2>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><strong></strong></p>
<ul>
<li>Small vocabulary in the training data
<ul>
<li>Gibberish in the results as only phonetic segments are recognised <em>Conservatoire</em>: <em>crosefwert oach</em> or <em>konssrwat dooane</em></li>
</ul></li>
<li>Compilation of a language model for Luxembourgish
<ul>
<li>Available text data (Chamber, RTL articles, 100,7 articles etc.)</li>
<li>77 Mio. word tokens</li>
<li>Extract ngrams (1-grams, 2-grams, …, 5-grams) and their frequency</li>
</ul></li>
<li>Language model adds probability for words to the acoustic model</li>
</ul>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><strong></strong></p>
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="Speech%20Recognition%20-%20Data%20Science%20and%20the%20Humanities%20-%202023_files/figure-revealjs/c9c84edb-659c-468a-8acb-8bff777b8074-1-9b2955a1-2122-4124-b7aa-3adf994ca44c.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="openais-whisper-radfordrobustspeechrecognition2022" class="slide level2">
<h2>OpenAI’s Whisper (<span class="citation" data-cites="radfordRobustSpeechRecognition2022">Radford et al. (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span>)</h2>
<ul>
<li><p>Introduced in September 2022</p></li>
<li><p>Sequence-to-sequence (seq2seq) architecture to learn the contextual representation of speech data</p></li>
<li><p>680,000 hours of training data from many languages, with transcriptions (e.g.&nbsp;YouTube videos)</p></li>
<li><p>‘weakly-supervised’ pretraining</p></li>
<li><p>Pre-training with up to 1.5 billion of parameters</p></li>
<li><p>Luxembourgish included as language</p>
<ul>
<li>with very bad quality, though ;-)</li>
</ul></li>
<li><p>Building blocks of Whisper <img data-src="Speech%20Recognition%20-%20Data%20Science%20and%20the%20Humanities%20-%202023_files/figure-revealjs/761e1554-3424-44c8-b4f8-3e203cf36f7e-1-4a62c4bf-6230-4349-85c7-3191cbad8d91.png"></p></li>
<li><p>Fine-tuning with our dataset</p></li>
</ul>
</section>
<section id="evaluation" class="slide level2">
<h2>Evaluation</h2>
<audio controls="" src="Chamber2022.wav">
</audio>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Ground truth</th>
<th>Wav2vec 2.0 (1B)</th>
<th>Whisper (large-v2)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Villmools merci, Här President. Den Avis vun den Experten huet kloer gewisen, datt d’Covid-Kris nach net eriwwer ass, dass de Risk nach ëmmer do ass an datt d’Expektative fir September, zumindest wat d’Experten ugeet, déi sinn, datt mer eventuell virun enger neier Well kënne stoen. Op wellechem Datum, wellech Variant, mat wellecher Virulenz, welleger Ustiechlegkeet, dat wësse mer an dësem Moment selbstverständlech net. Déi Zuelen, déi mer haut kennen, soen dat selwecht. De Staatsminister huet gëschter zitéiert: 1200 Infektiounen den Dag. Dat ass eppes, muss ech soen, wat eis virun enger Rei Joer, virun enger Rei Méint jo weesentlech méi erschreckt huet, wéi et eis haut erschreckt, well d’Situatioun</td>
<td>villmools merci här president den avis vun den experten huet kloer gewisen datt d’covidkris nach net eriwwer ass dass de risk nach ëmmer do ass an datt d’expektative fir september zumindest wat d’experten ugeet déi sinn datt mer eventuell virun enger neier well kënne stoen op wellechem datum <strong>well ech</strong> variant mat wellecher virulenz wellecher <strong>ustieche keet</strong> dat wësse mer an dësem moment selbstverständlech net déi zuelen déi mer haut kennen soen dat selwecht de staatsminister huet gëschter zitéiert den <strong>auszweehonnert</strong> infektiounen den dag dat ass eppes muss ech soen wat eis virun enger rei <strong>t eis</strong> haut erschreckt <strong>war</strong> d’situatioun</td>
<td>Villmools merci, Här President. Den Avis vun den Experten huet kloer gewisen, datt d’Covidkris nach net eriwwer ass, dass de Risk nach ëmmer do ass an datt d’Expektative fir September zumindest wat d’Experten ugeet, déi sinn, datt mer eventuell virun enger neier Well kënne stoen. Op <strong>wellegem</strong> Datum, <strong>welleg</strong> Variant, mat <strong>welleger</strong> Virulenz, welleger Ustiech<strong>tegkeet</strong>, dat wësse mer an dësem Moment selbstverständlech net. Déi Zuelen, déi mer haut kennen, soen dat selwecht. De Staatsminister huet gëschter zitéiert: 1200 Infektiounen den Dag. Dat ass eppes, muss ech soen, wat eis virun enger Rei Joer, virun enger Rei Méint jo weesentlech méi erschreckt huet, wéi et eis haut erschreckt, well d’Situatioun</td>
</tr>
</tbody>
</table>
</section>
<section id="evaluation-1" class="slide level2">
<h2>Evaluation</h2>
<ul>
<li>Word Error Rates (WER)
<ul>
<li>wav2vec 2.0: 10</li>
<li>Whisper: 13</li>
</ul></li>
<li>However, superior performance of Whisper over wav2vec 2.0
<ul>
<li>less errors</li>
<li>numbers recognised</li>
<li>capitalisation and punctuation restored</li>
</ul></li>
</ul>
</section>
<section id="multilingual-asr" class="slide level2">
<h2>Multilingual ASR</h2>
<audio controls="" src="Grand_Duc_2018.wav">
</audio>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Wav2vec 2.0</th>
<th>Whisper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>mir sollen houfreg sinn op d’ diversitéit an den zesummenhalt an eiser gesellschaft <strong>a setandroivoudr remercier les non luxembourgeois kee résidu qui travaille en entrep pour leur contribution précieuse an neutre société la cohésion économique mee aus si la sociale den autrepéys sont des atouts kinsapartier de défense dat oury i sont d’ acer den entree tenure reussite senatus</strong> haut op dësem chrescht owent wëll ech meng unerkennung awer net nëmmen op de politesche plang begrenzen</td>
<td>Mir sollen houfreg sinn op d’Diversitéit an den Zesummenhalt an eise Gesellschaft. <strong>A cet endroit, je voudrais remercier les non luxembourgeois qui resident ou qui travaillent à notre pays pour leur contribution précieuse à notre société. cohésion économique, mais aussi la cohésion sociale de notre pays sont des atouts qui nous appartiennent de défendre à tout prix. Ils sont au coeur de notre projet et de notre réussite. C’ est notre bien commun à tous.</strong> Haut, op dësem chrëschtel Wënd, wëll ech meng Unerkennung awer net nëmmen op de politesche Plang begrenzen.</td>
</tr>
</tbody>
</table>
</section>
<section id="demos" class="slide level2">
<h2>Demos</h2>
<ul>
<li>Available online at Hugging Face: (https://huggingface.co/unilux)</li>
</ul>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-baevskiWav2vecFrameworkSelfSupervised2020" class="csl-entry" role="listitem">
Baevski, Alexei, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. <span>“Wav2vec 2.0: <span>A</span> <span>Framework</span> for <span>Self</span>-<span>Supervised</span> <span>Learning</span> of <span>Speech</span> <span>Representations</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2006.11477">https://doi.org/10.48550/arXiv.2006.11477</a>.
</div>
<div id="ref-boigneTimelineLargeTransformer2021" class="csl-entry" role="listitem">
Boigne, Jonathan. 2021. <span>“A <span>Timeline</span> of <span>Large</span> <span>Transformer</span> <span>Models</span> for <span>Speech</span>.”</span> <em>Jonathan Bgn</em>. <a href="https://jonathanbgn.com/2021/12/31/timeline-transformers-speech.html">https://jonathanbgn.com/2021/12/31/timeline-transformers-speech.html</a>.
</div>
<div id="ref-FineTuneWav2Vec2English" class="csl-entry" role="listitem">
<span>“Fine-<span>Tune</span> <span>Wav2Vec2</span> for <span>English</span> <span>ASR</span> in <span>Hugging</span> <span>Face</span> with 🤗 <span>Transformers</span>.”</span> n.d. Accessed May 9, 2023. <a href="https://huggingface.co/blog/fine-tune-wav2vec2-english">https://huggingface.co/blog/fine-tune-wav2vec2-english</a>.
</div>
<div id="ref-huiSpeechRecognitionFeature2019" class="csl-entry" role="listitem">
Hui, Jonathan. 2019. <span>“Speech <span>Recognition</span> — <span>Feature</span> <span>Extraction</span> <span>MFCC</span> &amp; <span>PLP</span>.”</span> <em>Medium</em>. <a href="https://jonathan-hui.medium.com/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9">https://jonathan-hui.medium.com/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9</a>.
</div>
<div id="ref-huiSpeechRecognitionPhonetics2022" class="csl-entry" role="listitem">
———. 2022. <span>“Speech <span>Recognition</span> — <span>Phonetics</span>.”</span> <em>Medium</em>. <a href="https://jonathan-hui.medium.com/speech-recognition-phonetics-d761ea1710c0">https://jonathan-hui.medium.com/speech-recognition-phonetics-d761ea1710c0</a>.
</div>
<div id="ref-radfordRobustSpeechRecognition2022" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. <span>“Robust <span>Speech</span> <span>Recognition</span> via <span>Large</span>-<span>Scale</span> <span>Weak</span> <span>Supervision</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2212.04356">https://doi.org/10.48550/arXiv.2212.04356</a>.
</div>
</div>
<div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="Speech Recognition - Data Science and the Humanities - 2023_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'smaller': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>