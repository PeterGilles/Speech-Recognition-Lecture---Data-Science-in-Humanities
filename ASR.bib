
@article{coto-solanoAdvancesCompletelyAutomated2021,
	title = {Advances in {Completely} {Automated} {Vowel} {Analysis} for {Sociophonetics}: {Using} {End}-to-{End} {Speech} {Recognition} {Systems} {With} {DARLA}},
	volume = {4},
	issn = {2624-8212},
	shorttitle = {Advances in {Completely} {Automated} {Vowel} {Analysis} for {Sociophonetics}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8498339/},
	doi = {10.3389/frai.2021.662097},
	abstract = {In recent decades, computational approaches to sociophonetic vowel analysis have been steadily increasing, and sociolinguists now frequently use semi-automated systems for phonetic alignment and vowel formant extraction, including FAVE (Forced Alignment and Vowel Extraction, ; Evanini et al., Proceedings of Interspeech, 2009), Penn Aligner (Yuan and Liberman, J. Acoust. Soc. America, 2008, 123, 3878), and DARLA (Dartmouth Linguistic Automation), (Reddy and Stanford, DARLA Dartmouth Linguistic Automation: Online Tools for Linguistic Research, 2015a). Yet these systems still have a major bottleneck: manual transcription. For most modern sociolinguistic vowel alignment and formant extraction, researchers must first create manual transcriptions. This human step is painstaking, time-consuming, and resource intensive. If this manual step could be replaced with completely automated methods, sociolinguists could potentially tap into vast datasets that have previously been unexplored, including legacy recordings that are underutilized due to lack of transcriptions. Moreover, if sociolinguists could quickly and accurately extract phonetic information from the millions of hours of new audio content posted on the Internet every day, a virtual ocean of speech from newly created podcasts, videos, live-streams, and other audio content would now inform research. How close are the current technological tools to achieving such groundbreaking changes for sociolinguistics? Prior work (Reddy et al., Proceedings of the North American Association for Computational Linguistics 2015 Conference, 2015b, 71‚Äì75) showed that an HMM-based Automated Speech Recognition system, trained with CMU Sphinx (), was accurate enough for DARLA to uncover evidence of the US Southern Vowel Shift without any human transcription. Even so, because that automatic speech recognition (ASR) system relied on a small training set, it produced numerous transcription errors. Six years have passed since that study, and since that time numerous end-to-end automatic speech recognition (ASR) algorithms have shown considerable improvement in transcription quality. One example of such a system is the RNN/CTC-based DeepSpeech from Mozilla (). (RNN stands for recurrent neural networks, the learning mechanism for DeepSpeech. CTC stands for connectionist temporal classification, the mechanism to merge phones into words). The present paper combines DeepSpeech with DARLA to push the technological envelope and determine how well contemporary ASR systems can perform in completely automated vowel analyses with sociolinguistic goals. Specifically, we used these techniques on audio recordings from 352 North American English speakers in the International Dialects of English Archive (IDEA
1
), extracting 88,500 tokens of vowels in stressed position from spontaneous, free speech passages. With this large dataset we conducted acoustic sociophonetic analyses of the Southern Vowel Shift and the Northern Cities Chain Shift in the North American IDEA speakers. We compared the results using three different sources of transcriptions: 1) IDEA‚Äôs manual transcriptions as the baseline ‚Äúground truth‚Äù, 2) the ASR built on CMU Sphinx used by Reddy et al. (Proceedings of the North American Association for Computational Linguistics 2015 Conference, 2015b, 71‚Äì75), and 3) the latest publicly available Mozilla DeepSpeech system. We input these three different transcriptions to DARLA, which automatically aligned and extracted the vowel formants from the 352 IDEA speakers. Our quantitative results show that newer ASR systems like DeepSpeech show considerable promise for sociolinguistic applications like DARLA. We found that DeepSpeech‚Äôs automated transcriptions had significantly fewer character error rates than those from the prior Sphinx system (from 46 to 35\%). When we performed the sociolinguistic analysis of the extracted vowel formants from DARLA, we found that the automated transcriptions from DeepSpeech matched the results from the ground truth for the Southern Vowel Shift (SVS): five vowels showed a shift in both transcriptions, and two vowels didn‚Äôt show a shift in either transcription. The Northern Cities Shift (NCS) was more difficult to detect, but ground truth and DeepSpeech matched for four vowels: One of the vowels showed a clear shift, and three showed no shift in either transcription. Our study therefore shows how technology has made progress toward greater automation in vowel sociophonetics, while also showing what remains to be done. Our statistical modeling provides a quantified view of both the abilities and the limitations of a completely ‚Äúhands-free‚Äù analysis of vowel shifts in a large dataset. Naturally, when comparing a completely automated system against a semi-automated system involving human manual work, there will always be a tradeoff between accuracy on the one hand versus speed and replicability on the other hand [Kendall and Joseph, Towards best practices in sociophonetics (with Marianna DiPaolo), 2014]. The amount of ‚Äúnoise‚Äù that can be tolerated for a given study will depend on the particular research goals and researchers‚Äô preferences. Nonetheless, our study shows that, for certain large-scale applications and research goals, a completely automated approach using publicly available ASR can produce meaningful sociolinguistic results across large datasets, and these results can be generated quickly, efficiently, and with full replicability.},
	urldate = {2022-11-04},
	journal = {Frontiers in Artificial Intelligence},
	author = {Coto-Solano, Rolando and Stanford, James N. and Reddy, Sravana K.},
	month = sep,
	year = {2021},
	pmid = {34632373},
	pmcid = {PMC8498339},
	pages = {662097},
	file = {PubMed Central Full Text PDF:/Users/peter.gilles/Documents/zotero/storage/EWLDJC5L/Coto-Solano et al. - 2021 - Advances in Completely Automated Vowel Analysis fo.pdf:application/pdf},
}

@misc{huiSpeechRecognitionPhonetics2022,
	title = {Speech {Recognition} ‚Äî {Phonetics}},
	url = {https://jonathan-hui.medium.com/speech-recognition-phonetics-d761ea1710c0},
	abstract = {Finding the core principle and focus is unexpectedly hard for new inventions. In deep learning (DL), many early efforts spend on neurons‚Ä¶},
	language = {en},
	urldate = {2023-05-09},
	journal = {Medium},
	author = {Hui, Jonathan},
	month = sep,
	year = {2022},
	file = {Snapshot:/Users/peter.gilles/Documents/zotero/storage/N837GQH3/speech-recognition-phonetics-d761ea1710c0.html:text/html},
}

@misc{huiSpeechRecognitionFeature2019,
	title = {Speech {Recognition} ‚Äî {Feature} {Extraction} {MFCC} \& {PLP}},
	url = {https://jonathan-hui.medium.com/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9},
	abstract = {Machine learning ML extracts features from raw data and creates a dense representation of the content. This forces us to learn the core‚Ä¶},
	language = {en},
	urldate = {2023-05-09},
	journal = {Medium},
	author = {Hui, Jonathan},
	month = sep,
	year = {2019},
	file = {Snapshot:/Users/peter.gilles/Documents/zotero/storage/QNMUN3D2/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9.html:text/html},
}

@misc{huiSpeechRecognitionGMM2019,
	title = {Speech {Recognition} ‚Äî {GMM}, {HMM}},
	url = {https://jonathan-hui.medium.com/speech-recognition-gmm-hmm-8bb5eff8b196},
	abstract = {Before the Deep Learning (DL) era for speech recognition, HMM and GMM are two must-learn technology for speech recognition. Now, there are‚Ä¶},
	language = {en},
	urldate = {2023-05-09},
	journal = {Medium},
	author = {Hui, Jonathan},
	month = sep,
	year = {2019},
	file = {Snapshot:/Users/peter.gilles/Documents/zotero/storage/UP65M6UJ/speech-recognition-gmm-hmm-8bb5eff8b196.html:text/html},
}

@misc{boigneTimelineLargeTransformer2021,
	title = {A {Timeline} of {Large} {Transformer} {Models} for {Speech}},
	url = {https://jonathanbgn.com/2021/12/31/timeline-transformers-speech.html},
	abstract = {The Transformer architecture is taking over speech and leading to increasingly larger models, following a trend that started with natural language processing.},
	language = {en},
	urldate = {2023-05-09},
	journal = {Jonathan Bgn},
	author = {Boigne, Jonathan},
	month = dec,
	year = {2021},
	file = {Snapshot:/Users/peter.gilles/Documents/zotero/storage/EYFGJT8N/timeline-transformers-speech.html:text/html},
}

@misc{FineTuneWav2Vec2English,
	title = {Fine-{Tune} {Wav2Vec2} for {English} {ASR} in {Hugging} {Face} with ü§ó {Transformers}},
	url = {https://huggingface.co/blog/fine-tune-wav2vec2-english},
	abstract = {We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-05-09},
	file = {Snapshot:/Users/peter.gilles/Documents/zotero/storage/B3YC35Q5/fine-tune-wav2vec2-english.html:text/html},
}

@misc{radfordRobustSpeechRecognition2022,
	title = {Robust {Speech} {Recognition} via {Large}-{Scale} {Weak} {Supervision}},
	url = {http://arxiv.org/abs/2212.04356},
	doi = {10.48550/arXiv.2212.04356},
	abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
	urldate = {2023-05-09},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
	month = dec,
	year = {2022},
	note = {arXiv:2212.04356 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/peter.gilles/Documents/zotero/storage/6GG8BI8U/Radford et al. - 2022 - Robust Speech Recognition via Large-Scale Weak Sup.pdf:application/pdf;arXiv.org Snapshot:/Users/peter.gilles/Documents/zotero/storage/ZU7AT7GQ/2212.html:text/html},
}

@misc{FacebookWav2vec2xlsr300mHugging2023,
	title = {facebook/wav2vec2-xls-r-300m ¬∑ {Hugging} {Face}},
	url = {https://huggingface.co/facebook/wav2vec2-xls-r-300m},
	abstract = {We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-05-09},
	month = feb,
	year = {2023},
	file = {Snapshot:/Users/peter.gilles/Documents/zotero/storage/8SDJ753E/wav2vec2-xls-r-300m.html:text/html},
}

@misc{nguyenIMPROVINGLUXEMBOURGISHSPEECH2022,
	title = {{IMPROVING} {LUXEMBOURGISH} {SPEECH} {RECOGNITION} {WITH} {CROSS}-{LINGUAL} {SPEECH} {REPRESENTATIONS}},
	copyright = {2023 IEEE},
	author = {Nguyen, Le Minh and Nayak, Shekhar and Coler, Matt.},
	year = {2022},
	keywords = {language modelling, Luxembourgish, multilingual speech recognition, under-resourced language, wav2vec 2.0 XLSR-53},
}

@misc{baevskiWav2vecFrameworkSelfSupervised2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	shorttitle = {wav2vec 2.0},
	url = {http://arxiv.org/abs/2006.11477},
	doi = {10.48550/arXiv.2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2023-05-09},
	publisher = {arXiv},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	month = oct,
	year = {2020},
	note = {arXiv:2006.11477 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/peter.gilles/Documents/zotero/storage/I5W4JC3Y/Baevski et al. - 2020 - wav2vec 2.0 A Framework for Self-Supervised Learn.pdf:application/pdf;arXiv.org Snapshot:/Users/peter.gilles/Documents/zotero/storage/WMA5ME3N/2006.html:text/html},
}
